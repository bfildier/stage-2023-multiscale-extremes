This note is here to explain how I cleaned the data for the SAM CACATOES comparisson

The SAM data that are here : "/bdd/MT_WORKSPACE/REMY/RCEMIP/SAM/300K/"
They contain 26 days, over an grid of shape (X:62, Y: 4)

The CACATOES data that are here "/bdd/MT_WORKSPACE/REMY/RCEMIP/SAM/300K/"
They contain 25 days+ 1 timestep, over a 32 times finer grid of shape (X: 2048, Y : 128)

For now I used only the first day of each and removed the 64 first x indexes of the CACATOES data as it would fit no square from the SAM data

# ds2 = xr.open_dataset(sam_dir_path+"rcemip_large_2048x128x74_3km_12s_300K_64.2Dcom_1.nc")
# ds2 = ds2.isel(time=slice(48*n_days)).isel(x=slice(64, None))

Then for the CACATOES data I recomputed the dimensions so that they would be int64, time hence represents half-hourly step

When I tried with almost all the days by importing .nc using Dask.array, it worked well but then it was a nightmare to coarsen the already chunked DataArrays
# ds2 = xr.open_mfdataset([sam_dir_path+"rcemip_large_2048x128x74_3km_12s_300K_64.2Dcom_1.nc", sam_dir_path+"rcemip_large_2048x128x74_3km_12s_300K_64.2Dcom_2.nc"], concat_dim = "time", combine = "nested")
# ds2   = xr.concat([ds2_1, ds2_2], dim = ['time', 'x', 'y']) ## Doesn't work because too big

slides at https://docs.google.com/presentation/d/1Qy9XFlTuBbiwGvJMYQmqNM-DdzAvHnP59ta2DUfH9nc/edit#slide=id.g209ee29064d_0_65